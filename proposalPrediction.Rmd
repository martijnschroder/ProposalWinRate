---
title: "Predicting proposal win rates"
author: "Martijn Schroder"
date: "27/04/2019"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
  html_notebook:
    fig_caption: yes
    toc: yes
---
```{r load required libraries}
library(tidyverse)
library(lubridate)
library(ggthemes)
library(tibble)
library(gridExtra)
library(knitr)
library(kableExtra)
```

Load the data:
```{r load dataset}
# Read the dataset
proposals <- read_csv("data/data.csv", col_names = TRUE)
```


# Executive summary
In management consulting competitiveness is high. Our firm competes actively with the 'Big 4' consultancies. Better understanding of the reasons why we win and lose proposal will give us an advantage in the market.

Our business makes decisions on our proposal management practice with the view to increase win rates. Until recently those decisions were made based on personal experiences and perception of what works and what doesn't.

In this assignment my aim is to try machine learning approaches to gain insights into what the data tells us about the relevant features that are good predictors of win and lose rates.

Given the low number of transactions and limited cleanliness of the data, the analysis of features that should underpin decisions around proposals is relatively ambiguous. This is the main challenge to work with.

> For obvious reasons the data is de identified and various columns have been recoded. This part of the data wrangling has not been shown.

## Goal
The goal of this assignment is to try and predict whether proposals will be won or lost, given a set of features.

## Key steps
The following steps were taken:

  1. Investigate data and clean
  2. Explore data and gain insights
  3. Study suitability of machine learning approaches:
  4. Interpret results
  
The models investigated were:

* glm
* K nearest neighbours
* trees
* neural networks

## Results
The high number of categorical variables made this a challenging tasks in terms of applying machine learning approaches. I experimented with factorised labels of the features, but found that analysis was time consuming, didn't work for a number of model fitting techniques and took significant processing times. In the end I quantified categorical features through [Weights Of Evidence (WOE)](https://www.rdocumentation.org/packages/InformationValue/versions/1.2.3/topics/WOE) to enable easy model fitting.

The most accuracte predictions were achieved with K nearest neighbours, although other models performed comparably well too. All models were hard to interpret in terms of the causes of win rates.

Most significant finding was that our dataset could be improved through adding features that are quantified. Added these features will allow for further exploration of predictors of win rates, but will require changes in our porposal management practice. Examples of new features include:

* Subjective estimation of degree of confidence to win a proposal by sector experts
* Subjective estimation of the strength of the relationship with the client
* Quantification of match between proposal ask and our skills and experience

After gathering production data with these new features, it will enable further study of good predictors of proposal win rates.

Most features in the dataset were found to be significant. The value (monetary) of the proposals was a significant predictor too. This makes little sense, as the value is matched to the specific ask in proposals for the amount of work conducted. Secondly market analysis has shown that our rates are comparible with compatitors. In future analysis this feature will need to be dropped, unless it is matched with reliable information about the clients expectations in terms of proposal values.

Overall it was an enjoyable expercise, which gave me good insights in how our consultancy is performing and what potential future improvements may look like.

K nearest neighbours was the easiest model to fit and the most accurate. This intuitively makes sense as my task was primarily a classification task of categorical features.

# Investigate data and clean
The data set is a raw export from the system used to manage opportunities. A csv export was obtained, which after de identification has the following structure:

```{r names and structure raw dataset, echo=TRUE, warning=FALSE}
# rename "stage" column to "result" for easier comprehension
names(proposals)[names(proposals) == "stage"] <- "result"

str(proposals, give.attr = FALSE) # show structure of data
```

## Description of dataset and cleaning
The meaning of the columns is as follows:
```{r create and show table explanation of dataset, echo=FALSE}
# Create a pretty table explaining the data in the dataset
df_col_names <- names(proposals)
df_meaning <- c("Name of the account, i.e. the client",
                "Proposal win / loss (1 = win, 0 = loss)",
                "Estimated value of the opportunity",
                "Creation date of the record",
                "Close date of the record",
                "High level practice group for the opportunity",
                "Business offer the opportunity falls under (e.g. analytics, consulting, project management)",
                "Sector of business the client falls under (e.g. education, health, finance)",
                "Segment of the sector the opportunity falls under (e.g. insurance or loans for sector finance",
                "Proposal director; in our practice we assign a proposal director and manager to each proposal",
                "Proposal manager",
                "Source of the opportunity, i.e. how did we know of it (e.g. approached by client, tender",
                "Level of competitiveness for the proposal (e.g. how was it sourced?)",
        
                "Three letter code for the account"
                )
df <- tibble(df_col_names, df_meaning)
names(df) <- c("Column", "Description")

# Print the pretty table
df %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

rm(df, df_col_names, df_meaning)
```

The data needs cleaning up. The following changes are made:

* Convert "amount" column from num to double
* Convert "creationDate" and "closeDate" to Date format

```{r re code data types, cache=TRUE}
# Change data types for amount, creationDate and closeDate
proposals$amount <- as.double(proposals$amount)
proposals$creationDate <- as.Date(proposals$creationDate, "%d/%m/%y")
proposals$closeDate <- as.Date(proposals$closeDate, "%d/%m/%y")
```


# Explore data and gain insights
The following section outlines data exploration and progressive fixing of data issues.

## Data integrity
The figure below shows the proportion of NA values in the columns of the dataset. 

```{r NA values in dataset, echo=FALSE}
# Step 1: create a tibble with column names and proportions of NA values per column
n <- nrow(proposals) # number of rows in dataset
sumNAbefore <- tibble(colSums(is.na(proposals)/n)) # proportions of NA values per column
sumNAbefore <- cbind(colnames(proposals), sumNAbefore) # concatenate the vectors
names(sumNAbefore) <- c("feature", "proportion") # rename the columns to human readible

# plot NA proportions
sumNAbefore %>% ggplot(aes(reorder(feature, -proportion), proportion)) +
  geom_bar(stat = "Identity") +
  coord_flip() +
  xlab("Dataset column") +
  ylab("Proportion of NA values") +
  ggtitle("Proportion of NA values in the dataset") +
  theme_light()

rm(n, sumNAbefore)
```

The following approaches to addressing the NA values are implemented:

* *competitiveness*: drop the column. Although it would be interesting to see the impact of competitiveness on proposals, too many data points are missing to make it useful in the overall analysis. Business practice changes will need to ensure this data is consistently gathered going forward as this feature seems highly relevant.

```{r drop competitiveness column, eval=FALSE, include=FALSE}
proposals <- proposals %>% select(-competitiveness)
```


* *Amount*: Replace missing amounts with overall mean for amount. A better way would be to calculate group means for amounts over for example client and offer.

The distribution of amount and boxplots for win and loss data show significant outliers for amount. Skewness in the data set can be seen to mean that we win lots of small proposals, against which the bigger wins stand out. Potentially predictive analysis will need to take into account potential differences in win rates given size of proposals. This would more effectively address the dataset and treat big proposals as somewhat different from small proposals.

For subsequent steps I decided to leave all data in the set and not treat big proposals as outliers, knowing this would impact accuracy of the models fitted.

```{r summary of proposals$amount, warning=FALSE}
summary(proposals$amount)

temp <- proposals
temp$result[temp$result == 1] <- "win"
temp$result[temp$result == 0] <- "loss"

temp %>%
  ggplot(aes(practice, amount)) +
  geom_boxplot() +
  theme_light() +
  facet_wrap(~result, ncol=2) +
  ggtitle("Win / Loss boxplots over amount") +
  xlab("Practices") +
  ylab("Amount")

rm(temp)
```


```{r replace missing amounts with overall group mean}
# replace missing amount values with overall group mean
amounts <- as.double(proposals$amount[!is.na(proposals$amount)])
avg_amount <- mean(amounts[amounts > 999]) # calculate overall avg with values greater than 999
proposals$amount[is.na(proposals$amount)] <- avg_amount # replace NAs with avg amount
proposals$amount[proposals$amount <= 999] <- avg_amount # replace amounts < 999 with avg_amount
rm(amounts, avg_amount)
```

* *Proposal director / manager*: there are only `r sum(is.na(proposals$director))` observations with NA values for proposal director and `r sum(is.na(proposals$manager))` for manager. We'll recode those to "unknown".

```{r remove observations with NA values for director and manager}
# observations with NA values for director or manager
proposals$director[is.na(proposals$director)] <- "unknown"
proposals$manager[is.na(proposals$manager)] <- "unknown"
```


* *Sector*: there are `r sum(is.na(proposals$source))` NA values. Substitute NA values with "unknown"

```{r change NA values for source into "unknown"}
proposals$sector[is.na(proposals$sector)] <- "unknown"
```

* *Segment*: there are `r sum(is.na(proposals$segment))` NA values. We can potentially match offer data for observations with segment data and substitute missing values. Alternatively we leave NA values. In our business proposal managers have the responsibility to enter proposal information into our system. Are there any managers who need to improve on data quality?

```{r fix NA values for segment}
proposals[is.na(proposals$segment), ] %>% select(account, practice, sector, segment, offer) %>% arrange(sector) %>% group_by(offer) %>% summarise(n = n()) %>% arrange(desc(n))

# It looks like there's no serial offenders. Just limited clarity of process
proposals[is.na(proposals$segment), ] %>% select(account, practice, sector, segment, offer, manager) %>% group_by(manager) %>% summarise(n = n()) %>% arrange(desc(n)) %>% filter(n>2) %>% ggplot(aes(reorder(manager, n), n)) + geom_bar(stat="Identity") + coord_flip()

# Fix the offending entries
proposals$segment[is.na(proposals$segment)] <- "unknown"
```

As can be seen, there is a case to target specific managers with training to ensure they improve their admin practices.

* *Source*: there are `r sum(is.na(proposals$sector))` NA values. A quick analysis shows that there is significant predictive value in source for win/loss rates. Data quality will need to be addressed to ensure better predictions in future, especially given the fact that the NA group mean win rate is over 70%.

```{r win/loss rates given source, echo=FALSE}
# proportion of winning proposals given source
sum_tab <- proposals %>%
  group_by(source, result) %>%
  summarise(n = n(), value = sum(amount)) %>%
  ungroup() %>%
  group_by(source) %>%
  mutate(p = n / sum(n))

p1 <- sum_tab %>%
  filter(result == 1) %>%
  ggplot(aes(reorder(source, p), p)) +
  coord_flip() +
  geom_bar(stat = "Identity") +
  xlab("Source of proposal") +
  ylab("Proportion of proposals won")

p2 <- sum_tab %>%
  filter(result == 1) %>%
  ggplot(aes(n, p, colour=source)) +
  geom_point() +
  xlab("Number of proposals") +
  ylab("Win rate")

grid.arrange(p1, p2, ncol = 2)
rm(p1, p2, sum_tab)
```

As can be seen, the variability of win rates given sources is based on small numbers of observations, with the exeption of source = NA and source2. Win rates ordered by number of observations they're based on shows the following the following:

```{r win rates ordered by number of observations, echo=FALSE}
# table of win rates given number of observations
proposals %>%
  group_by(source, result) %>%
  summarise(n = n(), value = sum(amount)) %>%
  ungroup() %>%
  group_by(source) %>%
  mutate(p_win = n / sum(n)) %>%
  filter(result == 1 & n > 9) %>%
  arrange(desc(n)) %>%
  select(n, p_win, source)

```
Source2 has significantly more observations than any other source (all n < 10 not printed). Source can be used as a predictor, however for source NA values in the dataset we could substitute with overall win rate for NAs. Alternatively we create a new source, called "unknown". That is the path we'll follow.

```{r substitute source NA values with "unknown"}
proposals$source[is.na(proposals$source)] <- "unknown"
```

* *code*: `r sum(is.na(proposals$code))` entries are NA values. Recode those to "unknown"

```{r remove NA values from code}
proposals$code[is.na(proposals$code)] <- "unknown"
```


## Data exploration
First some basic exploration of the data to get an understanding of what it tells us.

* **Average win rate by account**
The graph below shows the average win rates for accounts with 10 or more proposal wins. Investigation of the higher win rates for accounts with larger numbers of proposals won would give important clues as to the reasons of their success.

```{r average win rate by account, echo=FALSE}
# average win rate by account
proposals %>%
  group_by(account, result) %>%
  summarise(n = n()) %>% # total number of wins and losses by account
  ungroup() %>%
  group_by(account) %>%
  mutate(p = n / sum(n)) %>% # proportions win/loss by account
  filter(n>9, result == 1) %>% # remove small n and select wins only
  arrange(desc(p)) %>%
  ggplot(aes(reorder(account, p), p, size=n)) +
  geom_point() + coord_flip() + theme_light() +
  xlab("Account") +
  ylab("Proposal win rate")
```

A number of follow up questions are interesting to pursue:

1. What are the differences between accounts in the win rate bins 0.4 - 0.6 and 0.6 - 0.8?
2. What are accounts doing differently in the win rate bin 0.8 - 1.0 and how does that relate to overall revenue generated?


* **Seasonality in win rates**: we experience seasonality in the number of proposals we respond to. The period before EOFY is usually busier. Let's explore number of proposals per year and win rates per year. The graph below shows the trends for the four practices in our business.

```{r explore seasonality, echo=FALSE}
# seasonal win rates by account and over practice
proposals %>%
  group_by(account, result) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(account) %>%
  mutate(p = n / sum(n)) %>%
  filter(result == 1) %>%
  select(-amount, -offer, -sector, -segment, -director, -manager) %>%
  arrange(desc(p)) %>%
  ggplot(aes(reorder(creationDate, p), p, colour=n, alpha = 0.4)) +
  geom_point() +
  facet_wrap(~ practice) +
  theme_minimal() +
  xlab("Date") +
  ylab("Proposal win rate")
```

Interesting features are that the practices over time seem to have improved win rates. A particularly interesting insight is the emergence of two sharp win rate lines at p = 0.5 and p = 1.0. This is not investigated further as part of this assignment, but one hypothesis is that this is related to recording practices that have changed at some point in time.


* **Do smaller jobs lead to bigger jobs?**
Investigate a time effect of jobs to jobs within an account

```{r do smaller jobs lead to bigger jobs, echo=FALSE}
# find the accounts with the most proposals won
proposals %>%
  group_by(account, result) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  top_n(10)

p1 <- proposals %>% filter(account == "account241") %>%
  ggplot(aes(creationDate, amount)) +
  geom_point() +
  theme_minimal()
  
p2 <- proposals %>%
  filter(result == 1) %>%
  group_by(account) %>%
  select(account, amount, creationDate, practice, offer, source) %>%
  mutate(n = n()) %>%
  arrange(desc(n)) %>%
  filter(n >= 49)

p2 <- p2 %>%
  ggplot(aes(creationDate, amount, alpha=0.4)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_minimal() +
  facet_wrap(~ account)

grid.arrange(p1, p2, ncol = 2)
rm(p1, p2)
```
As can be seen, the assumption that small jobs lead to bigger jobs seems implausible, at least for the 4 accounts selected in this plot with the most proposals won. Further analysis across all accounts could outline grouping for accounts in terms of their trending on value of proposals won.

* **How does proposal size relates to number of proposals won by account?**
```{r proposals size over proposals won by account, echo=FALSE, warning=FALSE}
# find the average size of proposals won and relate this to the amount of proposals submitted
p1 <- proposals %>%
  filter(result == 1) %>%
  group_by(account) %>%
  summarise(n = n(), avg_size = sum(amount) / n) %>%
  ggplot(aes(n, avg_size)) +
  geom_point() +
  geom_smooth(method = "loess") +
  theme_minimal() +
  xlab("Number of opps won per account") +
  ylab("Average amount of the opps")

p2 <- proposals %>%
  filter(result == 1) %>%
  group_by(account) %>%
  summarise(n = n(), size = sum(amount)) %>%
  ggplot(aes(n, size)) +
  geom_point() +
  geom_smooth(method = "loess") +
  theme_minimal() +
  xlab("Number of opps won per account") +
  ylab("Total amount of the opps per account")

grid.arrange(p1, p2, ncol = 2)
rm(p1, p2)
```

It looks like there's no effect on the number of successful proposals per account and the average amount per proposal won. 


# Study suitability of machine learning approaches
To study various machine learning approaches, I first split data in a train and test set. The train set contains 90% of the data. Given that we're fitting a binomial target variable it would be better to ensure that train and test sets have win/loss rates that match to overall proportion in the dataset. In this analysis I use a 50/50 split.

Each analysis will start with a fresh copy of the train and test data to ensure a clean run.

```{r create train and test set}
# Create train and test set with standard seed for reproducibiliy
set.seed(1)

# Get a random sample of 90% for the train set and 10% for validation
train_index <- sample(1:nrow(proposals), round(0.9*nrow(proposals)))
train_set_orig <- proposals[train_index, ]
test_set_orig <- proposals[-train_index, ]

# clean up environment
rm(train_index) # clean up
```

## glm
General linear regression is a powerful technique that fits binomial target variables. In order to apply regression (as opposed to MANOVA), I opted to quantify categorical features using Weights of Evidence (WOE). 

The generic glm model is as follows:
$$
\hat{Y} = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... \beta_kX_k
$$
with $\beta s$ the coefficients and $X s$ the prediciton variables.

To fit the glm model we

* Copy a fresh tran and test set
* factorise and apply WOE
* fit the model

```{r glm prepare train and test sets, include=FALSE}
train_set <- train_set_orig
test_set <- test_set_orig

# Factorise train_set
train_set$account <- factor(train_set$account)
train_set$result <- factor(train_set$result)
train_set$practice <- factor(train_set$practice)
train_set$offer <- factor(train_set$offer)
train_set$sector <- factor(train_set$sector)
train_set$segment <- factor(train_set$segment)
train_set$director <- factor(train_set$director)
train_set$manager <- factor(train_set$manager)
train_set$source <- factor(train_set$source)

# Factorise test_set
test_set$account <- factor(test_set$account)
test_set$result <- factor(test_set$result)
test_set$practice <- factor(test_set$practice)
test_set$offer <- factor(test_set$offer)
test_set$sector <- factor(test_set$sector)
test_set$segment <- factor(test_set$segment)
test_set$director <- factor(test_set$director)
test_set$manager <- factor(test_set$manager)
test_set$source <- factor(test_set$source)

library(InformationValue) # used for WOE function
# WOE train_set
train_set$account <- WOE(X=train_set$account, Y=train_set$result)
train_set$practice <- WOE(X=train_set$practice, Y=train_set$result)
train_set$offer <- WOE(X=train_set$offer, Y=train_set$result)
train_set$sector <- WOE(X=train_set$sector, Y=train_set$result)
train_set$segment <- WOE(X=train_set$segment, Y=train_set$result)
train_set$director <- WOE(X=train_set$director, Y=train_set$result)
train_set$manager <- WOE(X=train_set$manager, Y=train_set$result)
train_set$source <- WOE(X=train_set$source, Y=train_set$result)
train_set$code <- WOE(X=train_set$code, Y=train_set$result)

# WOE test_set
test_set$account <- WOE(X=test_set$account, Y=test_set$result)
test_set$practice <- WOE(X=test_set$practice, Y=test_set$result)
test_set$offer <- WOE(X=test_set$offer, Y=test_set$result)
test_set$sector <- WOE(X=test_set$sector, Y=test_set$result)
test_set$segment <- WOE(X=test_set$segment, Y=test_set$result)
test_set$director <- WOE(X=test_set$director, Y=test_set$result)
test_set$manager <- WOE(X=test_set$manager, Y=test_set$result)
test_set$source <- WOE(X=test_set$source, Y=test_set$result)
test_set$code <- WOE(X=test_set$code, Y=test_set$result)

# factorise target variable
train_set$result <- factor(train_set$result)
# remove date columns
train_set <- train_set %>% select(-creationDate, -closeDate, -code)
test_set <- test_set %>% select(-creationDate, -closeDate, -code)

```

The following model was fitted (creationDate, closeDate and code were removed for reasons of implausible causation and in the case of "code" a redundant feature as it was originally obtained from the account feature):
> model <- glm(result ~., family=binomial(link='logit'), data=train_set)

```{r fit the glm model, echo=FALSE, warning=FALSE}
# fit the model
model <- glm(result ~., family=binomial(link='logit'), data=train_set)
```

The summary statistics of the model show that all features are statistically significant, although "practice" is weak. Interestingly enough, "amount" is significant, which I find harder to interpret. It will be good to plot win rates over amounts of proposals and see if we do better or worse given proposal value.

```{r summary glm model, echo=FALSE}
summary(model)
```

We obtain predictions for the test dataset and determine to optimal cutoff for the test set and subsequently show the confusion matrix and overall accuracy of the model.

```{r predict with fitted glm model, echo=FALSE, warning=FALSE}
# plogis to ensure the predictions are 0 or 1
predict <- plogis(predict(model, test_set))

# find the optimal cutoff value
optCutOff <- optimalCutoff(test_set$result, predict)[1]

# misclassification error
# misClassError(test_set$result, predict, threshold = optCutOff)

# plotROC(test_set$result, predict) # the curve doesn't rise very steep. Model is not optimal

```

The confusion matrix shows reasonable results for the wins, but poor results for the losses. Overall, and despite the accuracy reported later, I find this result undesirable.

```{r glm confusion matrix}
# plot the confusion matrix. Prediction of wins is better than prediction of losses
confusionMatrix(test_set$result, predict, threshold = optCutOff)
```

Determine binomial prediction values (e.g. '0's and '1's) and report overall accuracy.

```{r glm real predictions and accuracy}
# calculate 0 and 1 results
realPredict <- ifelse(predict < optCutOff, 0, 1)

# accuracy
glm_accuracy <- sum(test_set$result == realPredict) / nrow(test_set)

# setup reporting environment
accuracy_results <- tibble(method = "glm fitting", accuracy = glm_accuracy)
accuracy_results

rm(model, glm_accuracy, optCutOff, predict, realPredict)
```

The accuracy is reasonable, but given the confusion matrix the specificity is poor.

One challenge with glm was that models trained with unaltered datasets (i.e. not using WOE and instead using lables) didn't generalise to observations with different labels (unless I assured that all feature labels (i.e. accountxyz) were in both train and test dataset). Doing this would come at a significant cost of reduced number of observations and reduced ability to preduct future proposals for new accounts / offers etc.

Instead I used WOE to ensure I

1. used all observations and
2. were able to generalise.

The cost of this decision was interpretability of the results and potentially changing how variance in the model is distributed. Thirdly it is much harder to see how causality works in proposal win rate prediction. And I'm a big fan of understand causality.

My learning is that the type of data collected may need to be expanded with features that are agnistic to offers, accounts and practices. This could include level of confidence to win the proposal, match between proposal request and skills and quality of the relationship.

Currently this information is not collected and would require a change in business practices and build up of production data.


## trees (rpart)
Trees are an intuitive way to think about proposal win rates. I was expecting to see super managers and directors (who determine win rates), offers that clearly outperformed and other such expecations. This did not happen. I felt that a few myths we like to hold true were slayed.

```{r trees prepare training and test datasets, include=FALSE}
train_set <- train_set_orig
test_set <- test_set_orig

# WOE the data
library(rpart)
library(InformationValue) # used for WOE function

# for WOE to work, the data needs to be factorised
# WOE train_set
train_set$account <- factor(train_set$account)
train_set$result <- factor(train_set$result)
train_set$practice <- factor(train_set$practice)
train_set$offer <- factor(train_set$offer)
train_set$sector <- factor(train_set$sector)
train_set$segment <- factor(train_set$segment)
train_set$director <- factor(train_set$director)
train_set$manager <- factor(train_set$manager)
train_set$source <- factor(train_set$source)

# WOE test_set
test_set$account <- factor(test_set$account)
test_set$result <- factor(test_set$result)
test_set$practice <- factor(test_set$practice)
test_set$offer <- factor(test_set$offer)
test_set$sector <- factor(test_set$sector)
test_set$segment <- factor(test_set$segment)
test_set$director <- factor(test_set$director)
test_set$manager <- factor(test_set$manager)
test_set$source <- factor(test_set$source)

# WOE train_set
train_set$account <- WOE(X=train_set$account, Y=train_set$result)
train_set$practice <- WOE(X=train_set$practice, Y=train_set$result)
train_set$offer <- WOE(X=train_set$offer, Y=train_set$result)
train_set$sector <- WOE(X=train_set$sector, Y=train_set$result)
train_set$segment <- WOE(X=train_set$segment, Y=train_set$result)
train_set$director <- WOE(X=train_set$director, Y=train_set$result)
train_set$manager <- WOE(X=train_set$manager, Y=train_set$result)
train_set$source <- WOE(X=train_set$source, Y=train_set$result)
train_set$code <- WOE(X=train_set$code, Y=train_set$result)

# WOE test_set
test_set$account <- WOE(X=test_set$account, Y=test_set$result)
test_set$practice <- WOE(X=test_set$practice, Y=test_set$result)
test_set$offer <- WOE(X=test_set$offer, Y=test_set$result)
test_set$sector <- WOE(X=test_set$sector, Y=test_set$result)
test_set$segment <- WOE(X=test_set$segment, Y=test_set$result)
test_set$director <- WOE(X=test_set$director, Y=test_set$result)
test_set$manager <- WOE(X=test_set$manager, Y=test_set$result)
test_set$source <- WOE(X=test_set$source, Y=test_set$result)
test_set$code <- WOE(X=test_set$code, Y=test_set$result)

```

The following code was run to fit the model:

```{r rpart tree fitting}

# fit the model with all featuers minus the date ones
fit <- rpart(result ~ .,
             method = "class",
             data = train_set)

# get predictions for test set
pred_test <- predict(fit, test_set, type = "class")

# calculate accuracy
pred <- tibble(pred_test)
real <- tibble(test_set$result)
trees_accuracy <- sum(pred == real) / nrow(pred)

```

A visual representation of the fitted model was challenging to interpret. The first significant decision point was amount, which makes little sense given that proposals request work of a particular size and the proposal value tries to match that. Future changes in business practices could be to add a feature expressing our confidence in matching price with understood expectations of the client.

I've fitted a second model without the "amount" feature. Results are reported below.


```{r fancy plot of tree model, echo=FALSE}
library(rattle)
library(rpart.plot)
library(RColorBrewer)

fancyRpartPlot(fit, sub = "Tree model with amount")

```


The accuracy of the tree model was slighty higher than the glm method.

```{r tree accuracy, echo=FALSE}
# report accuracy
accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="trees model with amount",  
                                     accuracy = trees_accuracy))

accuracy_results %>% knitr::kable()

```

Model fit without "amount".

```{r tree fit without amount, include=FALSE}
train_set <- train_set %>% select(-amount, -creationDate, - closeDate)
test_set <- test_set %>% select(-amount, -creationDate, - closeDate)

fit <- rpart(result ~ .,
             method = "class",
             data = train_set)

# get predictions for test set
pred_test <- predict(fit, test_set, type = "class")

# calculate accuracy
pred <- tibble(pred_test)
real <- tibble(test_set$result)
trees_accuracy <- sum(pred == real) / nrow(pred)
```

Visual of the model. This model is easier to interpret. The manager seems to have a large impact on the proposal win rate. This is an expected result and one deserving more analysis. The key future questions to explore are which managers are better at winning proposals and what do they do different?

```{r plot of tree model without amount, echo=FALSE}
fancyRpartPlot(fit, sub = "tree model without amount")
```

With accuracy:
```{r tree model accuracy without amount}
# report accuracy
accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="trees model without amount",  
                                     accuracy = trees_accuracy))

accuracy_results %>% knitr::kable()
```


```{r clean up tree environment}
# clean up environment
rm(pred, real, fit, train_set, test_set, pred_test, trees_accuracy)
```

## K nearest neighbours
K nearest neighbours aims to categorise observations based on distance of observations from group means of neighbouring abservations.

```{r knn prepare training and test datasets, include=FALSE}
# prepare datasets
train_set <- train_set_orig %>%
  na.omit() %>%
  select(-creationDate, -closeDate, -code)

test_set <- test_set_orig %>%
  na.omit() %>%
  select(-creationDate, -closeDate, -code)

# factorise the data to WOE it
train_set$account <- factor(train_set$account)
train_set$practice <- factor(train_set$practice)
train_set$offer <- factor(train_set$offer)
train_set$sector <- factor(train_set$sector)
train_set$segment <- factor(train_set$segment)
train_set$director <- factor(train_set$director)
train_set$manager <- factor(train_set$manager)
train_set$source <- factor(train_set$source)

# WOE test_set
test_set$account <- factor(test_set$account)
test_set$practice <- factor(test_set$practice)
test_set$offer <- factor(test_set$offer)
test_set$sector <- factor(test_set$sector)
test_set$segment <- factor(test_set$segment)
test_set$director <- factor(test_set$director)
test_set$manager <- factor(test_set$manager)
test_set$source <- factor(test_set$source)

# WOE the train_set
# don't WOE result column as that is the target
train_set$account <- WOE(X=train_set$account, Y=train_set$result)
train_set$practice <- WOE(X=train_set$practice, Y=train_set$result)
train_set$offer <- WOE(X=train_set$offer, Y=train_set$result)
train_set$sector <- WOE(X=train_set$sector, Y=train_set$result)
train_set$segment <- WOE(X=train_set$segment, Y=train_set$result)
train_set$director <- WOE(X=train_set$director, Y=train_set$result)
train_set$manager <- WOE(X=train_set$manager, Y=train_set$result)
train_set$source <- WOE(X=train_set$source, Y=train_set$result)

# WOE test_set
# don't WOE result column as that is the target
test_set$account <- WOE(X=test_set$account, Y=test_set$result)
test_set$practice <- WOE(X=test_set$practice, Y=test_set$result)
test_set$offer <- WOE(X=test_set$offer, Y=test_set$result)
test_set$sector <- WOE(X=test_set$sector, Y=test_set$result)
test_set$segment <- WOE(X=test_set$segment, Y=test_set$result)
test_set$director <- WOE(X=test_set$director, Y=test_set$result)
test_set$manager <- WOE(X=test_set$manager, Y=test_set$result)
test_set$source <- WOE(X=test_set$source, Y=test_set$result)

```

Model fitting.
```{r K nearest neighbours, message=FALSE, warning=FALSE, cache=TRUE}
# K nearest neighbours
library(caret)

train_set$result <- factor(train_set$result)
test_set$result <- factor(test_set$result)

train_knn <- train(result ~ ., method = "knn", 
                   data = train_set,
                   na.action = na.pass,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))

# best tune
#train_knn$bestTune

knn_accuracy <- confusionMatrix(predict(train_knn, test_set, type = "raw"),
                test_set$result)$overall["Accuracy"]

```

The following confusion matrix and accuracy was achieved.
```{r knn accuracy}

as.table(confusionMatrix(predict(train_knn, test_set, type = "raw"),
                test_set$result))

accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="K nearest neighbours",  
                                     accuracy = knn_accuracy))
accuracy_results %>% knitr::kable()

# clean up
rm(knn_accuracy, train_knn)
```

Results were slightly worse than other models fitted. The confusion matrix shows again that it is harder to predict losses than wins.

## Neural networks
Neural networks can be used for classification tasks susch as proposal win rates. Neural network topologies have an input layer (one neuron for each feature, one or more hidden layers and an output layer for the target variable).

```{r nn fresh data set, include=FALSE}
library(InformationValue) # used for WOE function

# All features are coded thorugh WOE
# For this to work, the features will need to be factorised

train_set <- train_set_orig
test_set <- test_set_orig

# for WOE to work, the data needs to be factorised
# WOE train_set
train_set$account <- factor(train_set$account)
train_set$practice <- factor(train_set$practice)
train_set$offer <- factor(train_set$offer)
train_set$sector <- factor(train_set$sector)
train_set$segment <- factor(train_set$segment)
train_set$director <- factor(train_set$director)
train_set$manager <- factor(train_set$manager)
train_set$source <- factor(train_set$source)
train_set$code <- factor(train_set$code)

# WOE test_set
test_set$account <- factor(test_set$account)
test_set$practice <- factor(test_set$practice)
test_set$offer <- factor(test_set$offer)
test_set$sector <- factor(test_set$sector)
test_set$segment <- factor(test_set$segment)
test_set$director <- factor(test_set$director)
test_set$manager <- factor(test_set$manager)
test_set$source <- factor(test_set$source)
test_set$code <- factor(test_set$code)


# WOE the train_set
# don't WOE result column as that is the target
train_set$account <- WOE(X=train_set$account, Y=train_set$result)
train_set$practice <- WOE(X=train_set$practice, Y=train_set$result)
train_set$offer <- WOE(X=train_set$offer, Y=train_set$result)
train_set$sector <- WOE(X=train_set$sector, Y=train_set$result)
train_set$segment <- WOE(X=train_set$segment, Y=train_set$result)
train_set$director <- WOE(X=train_set$director, Y=train_set$result)
train_set$manager <- WOE(X=train_set$manager, Y=train_set$result)
train_set$source <- WOE(X=train_set$source, Y=train_set$result)
train_set$code <- WOE(X=train_set$code, Y=train_set$result)

# WOE test_set
# don't WOE result column as that is the target
test_set$account <- WOE(X=test_set$account, Y=test_set$result)
test_set$practice <- WOE(X=test_set$practice, Y=test_set$result)
test_set$offer <- WOE(X=test_set$offer, Y=test_set$result)
test_set$sector <- WOE(X=test_set$sector, Y=test_set$result)
test_set$segment <- WOE(X=test_set$segment, Y=test_set$result)
test_set$director <- WOE(X=test_set$director, Y=test_set$result)
test_set$manager <- WOE(X=test_set$manager, Y=test_set$result)
test_set$source <- WOE(X=test_set$source, Y=test_set$result)
test_set$code <- WOE(X=test_set$code, Y=test_set$result)
```


The simplified formula describing predictions through neural networks is:
$$
Y=\sum(weight*input)+bias
$$
Y:      prediction
weight: strength of connections between neurons
input:  input signals of features
bias:   bias introduced

Definition of topologies can be complex and includes mechanisms to process signals, feedback loops to previous layers or neurons within a layer, "reward and punish" functions to train weights and more.

For my approach I'm using an input, hidden and output layer. I'm experimenting with varying numbers of neurons in the hidden layer and test accuracy as an overall performance measure.

```{r neural networks prepare train and test set, message=FALSE, warning=FALSE}
# Load the required libraries
library(neuralnet) # neural network engine


# create a function for reporting accuracy of the fitted models
nn_accuracy <- function(nn, test_set) {
  # Get the predictions for the test set
  Predict=neuralnet::compute(nn,test_set)

  # Converting probabilities into binary classes setting threshold level 0.5
  prob <- Predict$net.result
  pred <- ifelse(prob>0.5, 1, 0)
  
  # accuracy of predictions
  sum(pred == test_set$result) / nrow(pred)
}

```

* Test the first model with 1 hidden layer of 3 neurons and show the topolgy. The model trained is "result ~ account + practice + offer + sector + segment + director + manager + source".

```{r nn model 1}
nn=neuralnet(formula = result ~ account + practice + offer + sector + segment + director + manager + source,
             data=train_set, hidden=3,act.fct = "logistic",
             err.fct = "sse",
             linear.output = FALSE)

plot(nn)
```

This model has the following accuracy:

```{r accuracy model 1}
# calculate model 1 accuracy
accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="Neural network 1: 3 hidden",  
                                     accuracy = nn_accuracy(nn, test_set)))
accuracy_results %>% knitr::kable()
```

Try a few other models:

1. 5 neurons in hidden layer
2. 1 neuron in hidden layer
3. backward propogation

```{r compute the nn models}
# 5 neurons in hidden layer
nn=neuralnet(result ~ account + practice + offer + sector + segment + director + manager + source,
             data=train_set, hidden=5, act.fct = "logistic",
             linear.output = FALSE)

accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="5 neurons in hidden layer",  
                                     accuracy = nn_accuracy(nn, test_set)))

# 1 neuron in hidden layer
nn=neuralnet(result ~ account + practice + offer + sector + segment + director + manager + source,
             data=train_set, hidden=1, act.fct = "logistic",
             linear.output = FALSE)

accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="1 neuron in hidden layer",  
                                     accuracy = nn_accuracy(nn, test_set)))

# backward propogation
nn=neuralnet(result ~ account + practice + offer + sector + segment + director + manager + source,
             data=train_set, hidden=2, algorith="rprop+", act.fct = "logistic",
             linear.output = FALSE)

accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="2 neurons in hidden layer, propogation",  
                                     accuracy = nn_accuracy(nn, test_set)))

accuracy_results %>% knitr::kable()
```

Results are ok, but fairly unpredictable. Neural networks are compelling for their ability to predict and similarities with our own brain physiology. However it is hard to conceptualise how decisions are reached. The interpretability of neural networks is low, perhaps adding to their charm.

```{r clean up}
rm(nn, train_set, test_set, accuracy_results, nn_accuracy)
```


# Conclusions
Taking a real life challenge such as predicting win rates of proposals for our company was very satisfying. The date quality was low. Limited number of quantified features combined with many missing values.

Making sense of the techniques to apply to the data set has been a journey. Categorisation techniques were suitable for the challenge. Quantification of categorical variables greatly improved my ability to build models. However, this comes at the cost of losing clarity about the categories being recoded and hence the interpretability of the models.

The following models were fitted with resulting accuracies:

accuracy_results %>% knitr::kable()
