---
title: "Predicting proposal win rates"
author: "Martijn Schroder"
date: "1/05/2019"
output:
  pdf_document:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    fig_caption: yes
    toc: yes
---
```{r load requiredlibraries, include=FALSE}
library(tidyverse)
library(lubridate)
library(ggthemes)
library(tibble)
library(gridExtra)
#library(caret)
library(knitr)
library(kableExtra)
```


```{r load dataset, include=FALSE}
# Read the dataset
proposals <- read_csv("data/data.csv", col_names = TRUE)
```


# Executive summary
In management consulting competitiveness is high. Our firm competes actively with the 'Big 4' consultancies. Better understanding of the reasons why we win and lose proposal will give us an advantage in the market.

Our business makes decisions on our proposal management practice with the view to increase win rates. Until recently those decisions were made based on personal experiences and perception of what works and what doesn't.

In this assignment my aim is to try machine learning approaches to gain insights into what the data tells us about the relevant features that are good predictors of win and lose rates.

Given the low number of transactions and limited cleanliness of the data, the analysis of features that should underpin decisions around proposals is relatively ambiguous. This is the main challenge to work with.

> For obvious reasons the data is de identified and various columns have been recoded. This part of the data wrangling has not been shown.

## Goal
The goal of this assignment is to try and predict wether proposals will be won or lost, given a set of features.

## Key steps
The following steps were taken:

  1. Investigate data and cleanse where required
  2. Prepare data and explore
  3. Study suitability of machine learning approaches and select a method
  4. Train chosen model and evaluate performance with training set
  5. Interpret results and draw conclusions

## Results
The high number of categorical variables made this a challenging tasks in terms of applying machine learning approaches.

Good predictions were achieved with fitting K nearest neighbours. Regression based approaches performed reasonable. All models were hard to interpret in terms of the causes of win rates.

Most significant finding was that our dataset could be improved through adding featuers that are quantified. Added these features will allow for further exploration of predictors of win rates, but will require changes in our porposal management practice. Examples of new features include:

* Subjective estimation of degree of confidence to win a proposal by sector experts
* Subjective estimation of the strength of the relationship with the client
* **more points**

After gathering production data, it will then be possible to study of those features are good predictors of proposal win rates.

The main reasons why we win proposals seem to include:

* Amount of the proposal
* 

Features I expected to have a clear impact, but didn't included:

* the directors and managers working on the proposal
* the amount of past proposals won as a preduction of future win rates
* **more points**

# Cleaning data
The data set is a raw export from the system used to manage opportunities. A csv export was obtained, which after de identification has the following structure:

```{r names and structure raw dataset, echo=TRUE}
str(proposals, give.attr = FALSE) # show structure of data
```

## Description of dataset and cleaning
The meaning of the columns is as follows:
```{r create and show table explanation of dataset, echo=FALSE}
# Create a pretty table explaining the data in the dataset

df_col_names <- names(proposals)
df_meaning <- c("Name of the account, i.e. the client",
                "Proposal win / loss (1 = win, 0 = loss)",
                "Estimated value of the opportunity",
                "Creation date of the record",
                "Close date of the record",
                "High level practice group for the opportunity",
                "Business offer the opportunity falls under (e.g. analytics, consulting, project management)",
                "Sector of business the client falls under (e.g. education, health, finance)",
                "Segment of the sector the opportunity falls under (e.g. insurance or loans for sector finance",
                "Proposal director; in our practice we assign a proposal director and manager to each proposal",
                "Proposal manager",
                "Source of the opportunity, i.e. how did we know of it (e.g. approached by client, tender",
                "Level of competitiveness of the opportunity (i.e. are we only party bidding or are there more?)",
                "Three letter code for the account"
                )
df <- tibble(df_col_names, df_meaning)
names(df) <- c("Column", "Description")

# Print the pretty table
df %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

rm(df, df_col_names, df_meaning)
```

## Fixing data formats
The data needs cleaning up. The following changes are made:

* Convert "amount" column from num to double
* Convert "creationDate" and "closeDate" to Date format

```{r re code data types, cache=TRUE}
# Change data types for amount, creationDate and closeDate
proposals$amount <- as.double(proposals$amount)
proposals$creationDate <- as.Date(proposals$creationDate, "%d/%m/%y")
proposals$closeDate <- as.Date(proposals$closeDate, "%d/%m/%y")
```


# Exploration
## Exploration of columns
Some basic exploration of the columns to better understand what information is useful, given business rules.

## Data integrity
The figure below shows the proportion of NA values in the columns of the dataset. 

```{r NA values in dataset, echo=FALSE}
# Step 1: create a tibble with column names and proportions of NA values per column
n <- nrow(proposals) # number of rows in dataset
sumNAbefore <- tibble(colSums(is.na(proposals)/n)) # proportions of NA values per column
sumNAbefore <- cbind(colnames(proposals), sumNAbefore) # concatenate the vectors
names(sumNAbefore) <- c("feature", "proportion") # rename the columns to human readible

# plot NA proportions
sumNAbefore %>% ggplot(aes(reorder(feature, -proportion), proportion)) +
  geom_bar(stat = "Identity") +
  coord_flip() +
  xlab("Dataset column") +
  ylab("Proportion of NA values") +
  ggtitle("Proportion of NA values in the dataset") +
  theme_light()

rm(n, sumNAbefore)
```

The following approaches to addressing the NA values are implemented:

* *competitiveness*: drop the column. Although it would be interesting to see the impact of competitiveness on proposals, too many data points are missing to make it useful in the overall analysis. Business practice changes will need to ensure this data is consistently gathered going forward
```{r drop competitiveness column}
proposals <- proposals %>% select(-competitiveness)
```


* *Amount*: Replace missing amounts with overall mean for amount, or if group means can be used (e.g. group means for *offer*). The distribution of amount and boxplots for win and loss data show significant outliers for amount. Potentially predictive analysis will need to take into account potential differences in win rates given amount of proposals
```{r summary of proposals$amount, warning=FALSE}
summary(proposals$amount)

temp <- proposals
temp$stage[temp$stage == 1] <- "win"
temp$stage[temp$stage == 0] <- "loss"

temp %>%
  ggplot(aes(practice, amount)) +
  geom_boxplot() +
  theme_light() +
  facet_wrap(~stage, ncol=2) +
  ggtitle("Win / Loss boxplots over amount") +
  xlab("Practices") +
  ylab("Amount")

rm(temp)
```


```{r replace missing amounts with overall group mean}
# replace missing amount values with overall group mean
amounts <- as.double(proposals$amount[!is.na(proposals$amount)])
avg_amount <- mean(amounts[amounts > 999]) # calculate overall avg with values greater than 999
proposals$amount[is.na(proposals$amount)] <- avg_amount # replace NAs with avg amount
proposals$amount[proposals$amount <= 999] <- avg_amount # replace amounts < 999 with avg_amount
rm(amounts, avg_amount)
```

* *Proposal director / manager*: there are only 3 observations with NA values for proposal director and manager. For simplicity sake we'll delete those.
```{r remove observations with NA values for director and manager}
# observations with NA values for director or manager
proposals[is.na(proposals$director) | is.na(proposals$manager), ]

# delete the offending observations
proposals <- proposals[!(is.na(proposals$director) | is.na(proposals$manager)), ]
```


* *Sector*: there are `r sum(is.na(proposals$source))` NA values. Substitute NA values with "unknown"
```{r change NA values for source into "unknown"}
proposals$sector[is.na(proposals$sector)] <- "unknown"
```

* *Segment*: there are `r sum(is.na(proposals$segment))` NA values. We can potentially match offer data for observations with segment data and substitute missing values. Alternatively we leave NA values. Proposal managers have the responsibility to enter proposal data in our system. Are there any serial offenders in terms of data quality?

```{r fix NA values for segment}
proposals[is.na(proposals$segment), ] %>% select(account, practice, sector, segment, offer) %>% arrange(sector) %>% group_by(offer) %>% summarise(n = n()) %>% arrange(desc(n))

# It looks like there's no serial offenders. Just limited clarity of process
proposals[is.na(proposals$segment), ] %>% select(account, practice, sector, segment, offer, manager) %>% group_by(manager) %>% summarise(n = n()) %>% arrange(desc(n)) %>% filter(n>2) %>% ggplot(aes(reorder(manager, n), n)) + geom_bar(stat="Identity") + coord_flip()

# Fix the offending entries
proposals$segment[is.na(proposals$segment)] <- "unknown"
```


* *Source*: there are `r sum(is.na(proposals$sector))` NA values. A quick analysis shows that there is significant predictive value in source for win/loss rates. Although many values are missing, we can try and see if a relation exists between source and other features:
   * 
```{r win/loss rates given source}
# proportion of winning proposals given source
sum_tab <- proposals %>%
  group_by(source, stage) %>%
  summarise(n = n(), value = sum(amount)) %>%
  ungroup() %>%
  group_by(source) %>%
  mutate(p = n / sum(n))

p1 <- sum_tab %>%
  filter(stage == 1) %>%
  ggplot(aes(reorder(source, p), p)) +
  coord_flip() +
  geom_bar(stat = "Identity") +
  xlab("Source of proposal") +
  ylab("Proportion of proposals won")

p2 <- sum_tab %>%
  filter(stage == 1) %>%
  ggplot(aes(n, p, colour=source)) +
  geom_point() +
  xlab("Number of proposals") +
  ylab("Win rate")

grid.arrange(p1, p2, ncol = 2)
rm(p1, p2, sum_tab)
```
As can be seen, the variability of win rates given sources is based on small numbers of observations. Win rates ordered by number of observations they're based on shows the following the following:

```{r win rates ordered by number of observations}
# table of win rates given number of observations
proposals %>%
  group_by(source, stage) %>%
  summarise(n = n(), value = sum(amount)) %>%
  ungroup() %>%
  group_by(source) %>%
  mutate(p_win = n / sum(n)) %>%
  filter(stage == 1) %>%
  arrange(desc(n)) %>%
  select(n, p_win, source)

```
Source2 has significantly more observations than any other sources (all n <= 10 not printed). Source can be used as a predictor, however for source NA values in the dataset we could substitute with overall win rate for NAs. Alternatively we create a new source, called "unknown". That is the path we'll follow.
```{r substitute source NA values with "unknown"}
proposals$source[is.na(proposals$source)] <- "unknown"
```

* *code*: `r sum(is.na(proposals$code))` entries are NA values. Recode those to "unknown"
```{r remove NA values from code}
proposals$code[is.na(proposals$code)] <- "unknown"
```


## Data exploration
First some basic exploration of the data to get an understanding of what it tells us.

* **Average win rate by account**
```{r average win rate by account}
# average win rate by account
proposals %>%
  group_by(account, stage) %>%
  summarise(n = n()) %>% # total number of wins and losses by account
  ungroup() %>%
  group_by(account) %>%
  mutate(p = n / sum(n)) %>% # proportions win/loss by account
  filter(n>9, stage == 1) %>% # remove small n and select wins only
  arrange(desc(p)) %>%
  ggplot(aes(reorder(account, p), p, size=n)) +
  geom_point() + coord_flip() + theme_light() +
  xlab("Account") +
  ylab("Proposal win rate")
```

A number of observations:

1. What are the differences between accounts with win rates between (0.6 and 0.8) and (0.8 and 1.0)?


* **Seasonality in win rates**: we experience seasonality in the number of proposals we respond to. Let's explore number of proposals per year and win rates per year

```{r explore seasonality}
# seasonal win rates by account and over practice
proposals %>%
  group_by(account, stage) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  group_by(account) %>%
  mutate(p = n / sum(n)) %>%
  filter(stage == 1) %>%
  select(-amount, -offer, -sector, -segment, -director, -manager) %>%
  arrange(desc(p)) %>%
  ggplot(aes(reorder(creationDate, p), p, colour=n, alpha = 0.4)) +
  geom_point() +
  facet_wrap(~ practice) +
  theme_minimal() +
  xlab("Date") +
  ylab("Proposal win rate")
```

* **Overall win rates over time**
```{r overall win rates over time}
# TODO
```

* **Do smaller jobs lead to bigger jobs?**
Investigate a time effect of jobs to jobs within an account

```{r do smaller jobs lead to bigger jobs}
# find the accounts with the most proposals won
proposals %>%
  group_by(account, stage) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  top_n(10)

p1 <- proposals %>% filter(account == "account241") %>%
  ggplot(aes(creationDate, amount)) +
  geom_point() +
  theme_minimal()
  
p2 <- proposals %>%
  filter(stage == 1) %>%
  group_by(account) %>%
  select(account, amount, creationDate, practice, offer, source) %>%
  mutate(n = n()) %>%
  arrange(desc(n)) %>%
  filter(n >= 49)

p2 <- p2 %>%
  ggplot(aes(creationDate, amount, alpha=0.4)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_minimal() +
  facet_wrap(~ account)

grid.arrange(p1, p2, ncol = 2)
rm(p1, p2)
```
As can be seen, the assumption that small jobs lead to bigger jobs seems implausible, at least for the 4 accounts with the most proposals won.

* **How does proposal size relates to number of proposals won by account?**
```{r proposals size over proposals won by account, warning=FALSE}
# find the average size of proposals won and relate this to the amount of proposals submitted
p1 <- proposals %>%
  filter(stage == 1) %>%
  group_by(account) %>%
  summarise(n = n(), avg_size = sum(amount) / n) %>%
  ggplot(aes(n, avg_size)) +
  geom_point() +
  geom_smooth(method = "loess") +
  theme_minimal() +
  xlab("Number of opps won per account") +
  ylab("Average amount of the opps")

p2 <- proposals %>%
  filter(stage == 1) %>%
  group_by(account) %>%
  summarise(n = n(), size = sum(amount)) %>%
  ggplot(aes(n, size)) +
  geom_point() +
  geom_smooth(method = "loess") +
  theme_minimal() +
  xlab("Number of opps won per account") +
  ylab("Total amount of the opps per account")

grid.arrange(p1, p2, ncol = 2)
rm(p1, p2)
```

It looks like there's no effect on the number of successful proposals per account and the average amount per proposal won. 


# Method

1. Try various ML approaches (prep data to suit the analysis)
2. Select potential model
3. Conduct analysis

# Analysis
Split data in a train and test set. The train set will contain 90% of the data.
```{r create train and test set}
# Create train and test set with standard seed for reproducibiliy
set.seed(1)

# Get a random sample of 90% for the train set and 10% for validation
train_index <- sample(1:nrow(proposals), round(0.9*nrow(proposals)))
train_set_orig <- proposals[train_index, ]
test_set_orig <- proposals[-train_index, ]

# clean up environment
rm(train_index) # clean up
```

## glm


## trees (rpart)

1. Apply WOE to data, to create continuous variables from categorical ones
Compute the Weights Of Evidence (WOE) for each group of a given categorical X and binary response Y. The resulting WOE can be usued in place of the categorical X so as to be used as a continuous variable.
```{r rpart tree fitting}
train_set <- train_set_orig
test_set <- test_set_orig

# WOE the data
library(rpart)
library(InformationValue) # used for WOE function

# for WOE to work, the data needs to be factorised
# WOE train_set
train_set$account <- factor(train_set$account)
train_set$stage <- factor(train_set$stage)
train_set$practice <- factor(train_set$practice)
train_set$offer <- factor(train_set$offer)
train_set$sector <- factor(train_set$sector)
train_set$segment <- factor(train_set$segment)
train_set$director <- factor(train_set$director)
train_set$manager <- factor(train_set$manager)
train_set$source <- factor(train_set$source)

# WOE test_set
test_set$account <- factor(test_set$account)
test_set$stage <- factor(test_set$stage)
test_set$practice <- factor(test_set$practice)
test_set$offer <- factor(test_set$offer)
test_set$sector <- factor(test_set$sector)
test_set$segment <- factor(test_set$segment)
test_set$director <- factor(test_set$director)
test_set$manager <- factor(test_set$manager)
test_set$source <- factor(test_set$source)

# WOE train_set
train_set$account <- WOE(X=train_set$account, Y=train_set$stage)
train_set$practice <- WOE(X=train_set$practice, Y=train_set$stage)
train_set$offer <- WOE(X=train_set$offer, Y=train_set$stage)
train_set$sector <- WOE(X=train_set$sector, Y=train_set$stage)
train_set$segment <- WOE(X=train_set$segment, Y=train_set$stage)
train_set$director <- WOE(X=train_set$director, Y=train_set$stage)
train_set$manager <- WOE(X=train_set$manager, Y=train_set$stage)
train_set$source <- WOE(X=train_set$source, Y=train_set$stage)
train_set$code <- WOE(X=train_set$code, Y=train_set$stage)

# WOE test_set
test_set$account <- WOE(X=test_set$account, Y=test_set$stage)
test_set$practice <- WOE(X=test_set$practice, Y=test_set$stage)
test_set$offer <- WOE(X=test_set$offer, Y=test_set$stage)
test_set$sector <- WOE(X=test_set$sector, Y=test_set$stage)
test_set$segment <- WOE(X=test_set$segment, Y=test_set$stage)
test_set$director <- WOE(X=test_set$director, Y=test_set$stage)
test_set$manager <- WOE(X=test_set$manager, Y=test_set$stage)
test_set$source <- WOE(X=test_set$source, Y=test_set$stage)
test_set$code <- WOE(X=test_set$code, Y=test_set$stage)

# fit the model with all featuers minus the date ones
fit <- rpart(stage ~ .,
             method = "class",
             data = train_set)

# get predictions for test set
pred_test <- predict(fit, test_set, type = "class")

# calculate accuracy
pred <- tibble(pred_test)
real <- tibble(test_set$stage)
sum(pred == real) / nrow(pred)

# clean up environment
rm(pred, real, fit, train_set, test_set, pred_test)
```
Best accuracy: 0.7801418
I bet that's better than humans would achieve.

## K nearest neighbours
K nearest neightbour performs well
```{r K nearest neighbours, cache=TRUE}
# K nearest neighbours
library(caret)

train_set <- train_set_orig %>%
  na.omit() %>%
  select(-creationDate, -closeDate, -account, -amount, -director, -manager, -code)

test_set <- test_set_orig %>%
  na.omit() %>%
  select(-creationDate, -closeDate, -account, -amount, -director, -manager, -code)

train_set$stage <- factor(train_set$stage)
test_set$stage <- factor(test_set$stage)

train_knn <- train(stage ~ ., method = "knn", 
                   data = train_set,
                   na.action = na.pass,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))

# best tune
train_knn$bestTune

confusionMatrix(predict(train_knn, test_set, type = "raw"),
                test_set$stage)$overall["Accuracy"]

```

Best accuracy: 0.8120567
Even better than the tree model!


## Neural networks
Neural networks can be used for classification tasks susch as proposal win rates. Neural network topologies have an input layer (one neuron for each feature, one or more hidden layers and an output layer for the target variable).

The simplified formula describing predictions through neural networks is:
$$
Y=\sum(weight*input)+bias
$$
Y:      prediction
weight: strength of connections between neurons
input:  input signals of features
bias:   bias introduced

Definition of topologies can be complex and includes mechanisms to process signals, feedback loops to previous layers or neurons within a layer, "reward and punish" functions to train weights and more.

For my approach I'm using an input, hidden and output layer. I'm experimenting with varying numbers of neurons in the hidden layer and test accuracy as an overall performance measure.

```{r prepare train and test set, include=FALSE}
# Load the required libraries
library(neuralnet) # neural network engine
library(InformationValue) # used for WOE function

# All features are coded thorugh WOE
# For this to work, the features will need to be factorised

train_set <- train_set_orig
test_set <- test_set_orig

# for WOE to work, the data needs to be factorised
# WOE train_set
train_set$account <- factor(train_set$account)
train_set$practice <- factor(train_set$practice)
train_set$offer <- factor(train_set$offer)
train_set$sector <- factor(train_set$sector)
train_set$segment <- factor(train_set$segment)
train_set$director <- factor(train_set$director)
train_set$manager <- factor(train_set$manager)
train_set$source <- factor(train_set$source)
train_set$code <- factor(train_set$code)

# WOE test_set
test_set$account <- factor(test_set$account)
test_set$practice <- factor(test_set$practice)
test_set$offer <- factor(test_set$offer)
test_set$sector <- factor(test_set$sector)
test_set$segment <- factor(test_set$segment)
test_set$director <- factor(test_set$director)
test_set$manager <- factor(test_set$manager)
test_set$source <- factor(test_set$source)
test_set$code <- factor(test_set$code)


# WOE the train_set
# don't WOE stage column as that is the target
train_set$account <- WOE(X=train_set$account, Y=train_set$stage)
train_set$practice <- WOE(X=train_set$practice, Y=train_set$stage)
train_set$offer <- WOE(X=train_set$offer, Y=train_set$stage)
train_set$sector <- WOE(X=train_set$sector, Y=train_set$stage)
train_set$segment <- WOE(X=train_set$segment, Y=train_set$stage)
train_set$director <- WOE(X=train_set$director, Y=train_set$stage)
train_set$manager <- WOE(X=train_set$manager, Y=train_set$stage)
train_set$source <- WOE(X=train_set$source, Y=train_set$stage)
train_set$code <- WOE(X=train_set$code, Y=train_set$stage)

# WOE test_set
# don't WOE stage column as that is the target
test_set$account <- WOE(X=test_set$account, Y=test_set$stage)
test_set$practice <- WOE(X=test_set$practice, Y=test_set$stage)
test_set$offer <- WOE(X=test_set$offer, Y=test_set$stage)
test_set$sector <- WOE(X=test_set$sector, Y=test_set$stage)
test_set$segment <- WOE(X=test_set$segment, Y=test_set$stage)
test_set$director <- WOE(X=test_set$director, Y=test_set$stage)
test_set$manager <- WOE(X=test_set$manager, Y=test_set$stage)
test_set$source <- WOE(X=test_set$source, Y=test_set$stage)
test_set$code <- WOE(X=test_set$code, Y=test_set$stage)

# create a function for reporting accuracy of the fitted models
nn_accuracy <- function(nn, test_set) {
  # Get the predictions for the test set
  Predict=neuralnet::compute(nn,test_set)

  # Converting probabilities into binary classes setting threshold level 0.5
  prob <- Predict$net.result
  pred <- ifelse(prob>0.5, 1, 0)
  
  # accuracy of predictions
  sum(pred == test_set$stage) / nrow(pred)
}

```

* Test the first model with 1 hidden layer of 3 neurons and show the topolgy. The model trained is "stage ~ account + practice + offer + sector + segment + director + manager + source".
```{r nn model 1}
nn=neuralnet(stage ~ account + practice + offer + sector + segment + director + manager + source,
             data=train_set, hidden=3,act.fct = "logistic",
             linear.output = FALSE)

plot(nn)
```

This model has the following accuracy:
```{r accuracy model 1}
# calculate model 1 accuracy
accuracy_results <- tibble(method = "Model 1: 3 hidden", accuracy = nn_accuracy(nn, test_set))
accuracy_results
```

Try a few other models:

1. 5 neurons in hidden layer
2. 1 neuron in hidden layer
3. backward propogation

```{r compute the nn models}
# 5 neurons in hidden layer
nn=neuralnet(stage ~ account + practice + offer + sector + segment + director + manager + source,
             data=train_set, hidden=5, act.fct = "logistic",
             linear.output = FALSE)

accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="5 neurons in hidden layer",  
                                     accuracy = nn_accuracy(nn, test_set)))

# 1 neuron in hidden layer
nn=neuralnet(stage ~ account + practice + offer + sector + segment + director + manager + source,
             data=train_set, hidden=1, act.fct = "logistic",
             linear.output = FALSE)

accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="1 neuron in hidden layer",  
                                     accuracy = nn_accuracy(nn, test_set)))

# backward propogation
nn=neuralnet(stage ~ account + practice + offer + sector + segment + director + manager + source,
             data=train_set, hidden=2, algorith="rprop+", act.fct = "logistic",
             linear.output = FALSE)

accuracy_results <- bind_rows(accuracy_results,
                          tibble(method="2 neurons in hidden layer, propogation",  
                                     accuracy = nn_accuracy(nn, test_set)))

accuracy_results
```
Results are ok, but fairly unpredictable. Neural networks are compelling for their ability to predict and similarities with our own brain physiology. However it is hard to conceptualise how decisions are reached. The interpretability of neural networks is low, perhaps adding to their charm.
```{r clean up}
rm(nn, pred, Predict, prob, train_set, test_set, accuracy_results, nn_accuracy)
```


# Conclusions
Taking a real life challenge such as predicting win rates of proposals for our company was very satisfying. The date quality was low. Limited number of quantified features combined with many missing values.

Making sense of the techniques to apply to the data set has been a journey. Categorisation techniques were suitable for the challenge. Quantification of categorical variables greatly improved my ability to build models. However, this comes at the cost of losing clarity about the categories being recoded and hence the interpretability of the models.

The following models were fitted with resulting accuracies:

| **model** | **accuracy** |
|glm|1|
|trees|2|
|K nearest neighbours|3|
|neural networks|4|
